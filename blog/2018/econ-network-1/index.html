<!DOCTYPE html> <html lang="en"> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>A snapshot of economics, part 1 - getting the data | Martin Stancsics</title> <meta name="author" content="Martin Stancsics"/> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "/> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light"/> <link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üíæ</text></svg>"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://stanmart.github.io/blog/2018/econ-network-1/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Martin¬†</span>Stancsics</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/papers/">papers</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">A snapshot of economics, part 1 - getting the data</h1> <p class="post-meta">October 25, 2018</p> <p class="post-tags"> <a href="/blog/2018"> <i class="fas fa-calendar fa-sm"></i> 2018 </a> ¬† ¬∑ ¬† <a href="/blog/tag/network"> <i class="fas fa-hashtag fa-sm"></i> network</a> ¬† <a href="/blog/tag/web"> <i class="fas fa-hashtag fa-sm"></i> web</a> ¬† <a href="/blog/tag/scraping"> <i class="fas fa-hashtag fa-sm"></i> scraping</a> ¬† <a href="/blog/tag/python"> <i class="fas fa-hashtag fa-sm"></i> python</a> ¬† <a href="/blog/tag/EconLit"> <i class="fas fa-hashtag fa-sm"></i> EconLit</a> ¬† <a href="/blog/tag/EBSCOhost"> <i class="fas fa-hashtag fa-sm"></i> EBSCOhost</a> ¬† </p> </header> <article class="post-content"> <p>Back in 2015 Olena Chystiakova and I did a project for a Network Science course that might be interesting from a web scraping or data visualization perspective. I am going to present this project in a series of three short articles: I will start with getting the data (web scraping), continue with the visualization of the network, and conclude with modelling the network (the actual network science stuff). This is the first part of the series.</p> <h2 id="the-task">The task</h2> <p>The project we had to do had quite loose requirements: we had to obtain a real-life network and then analyze it. As is often the case when getting such tasks, the most difficult part was deciding what exactly to analyze. In the end, we settled on a network representing economics: the subfields of economics, connected by articles that belong to multiple subfields.</p> <p>Fortunately, the American Economic Association has a system for classifying academic content in Economics: the system of <a href="https://www.aeaweb.org/econlit/jelCodes.php?view=jel" target="_blank" rel="noopener noreferrer">JEL (Journal of Economic Literature)</a> codes. The classifications are hierarchical: the codes consist of a letter (main category) followed by two numbers (subcategories). An example JEL code is the following:</p> <ul> <li>J: Labor and demographic economics <ul> <li>J2: Time Allocation, Work Behavior, and Employment Determination and Creation; Human capital <ul> <li>J22: Time allocation and Labor supply</li> </ul> </li> </ul> </li> </ul> <p>Almost all of the articles published in the field of economics (and most of the working papers too) are associated with usually 2-5 (self-declared) JEL codes. With this information in mind, the layout of the network is clear:</p> <ul> <li>nodes will represent the subfields of economics (JEL codes), and</li> <li>edges will represent articles in which a pair of JEL codes is featured simultaneously (or more specifically, the weight of the edges will represent the number of papers in which a pair of JEL codes is featured together).</li> </ul> <p>All we need now is the data: a bunch of articles identifiers with the associated JEL codes.</p> <h2 id="getting-the-data">Getting the data</h2> <h3 id="can-we-just-scrape-it">Can we just scrape it?</h3> <p>At the time of this project we were studying at the Central European University, and therefore had access to <em>EconLit</em>, an index from the American Economic Association. The problem was that the service the university signed up for was designed for searching articles through services like <em>EBSCOhost</em> and viewing them one-by one, not for downloading bulk information. But if we can view them one-by-one, Python can also do the same. There is no need for us to sit there, it is a classic job for web scraping.</p> <p>Or is it?</p> <p>Regarding web scraping, the first question should always be whether we can do it. Not in the sense that ‚Äòare we capable to do it?‚Äô - we most likely are. It might take a bit of poking around and trial and error, but (with the possible exception of CAPTCHAs and some other advanced anti-scraping measures) a script can usually simulate a human well enough to in order to harvest data. The more difficult (for most peaople trying to write a web scraper anyways) question is legal: are we allowed to do it?</p> <p>The problem is that web scraping is often a legal grey zone. Contrary to common sense, harvesting the data from a webpage using a script might be legally problematic even if doing so by hand would be perfectly fine. And in some cases, even doing it by hand may be prohibited! Also, the consequences can be quite serious if the owner of the data decides to seek legal action. <a href="https://benbernardblog.com/web-scraping-and-crawling-are-perfectly-legal-right/" target="_blank" rel="noopener noreferrer">This article</a> is a highly recommended read for anyone considering web scraping.</p> <p>To see if we are in the clear with this project, let‚Äôs start with the obvious: reading the <a href="https://www.ebsco.com/terms-of-use" target="_blank" rel="noopener noreferrer">terms of service</a>. For us, the relevant part is I.C:</p> <blockquote> <p>Licensee and Authorized Users agree to abide by the Copyright Act of 1976 [‚Ä¶] <strong>Downloading all or parts of the Databases or Services in a systematic or regular manner</strong> so as to create a collection of materials comprising all or part of the Databases or Services <strong>is strictly prohibited</strong> whether or not such collection is in electronic or print form. <strong>Notwithstanding the above restrictions, this paragraph shall not restrict the use of the materials under the doctrine of ‚Äúfair use‚Äù</strong> as defined under the laws of the United States. Publishers may impose their own conditions of use applicable only to their content. [‚Ä¶]</p> </blockquote> <p>There are two important parts here:</p> <ul> <li>According to the first part, what we are trying to do is strictly prohibited.</li> <li>However, the first part does not restrict the use of materials falling under <em>fair use</em>.</li> </ul> <p>Now <a href="https://guides.nyu.edu/fairuse" target="_blank" rel="noopener noreferrer">fair use</a> is a grey area in itself: it has no well defined boundaries, and the actual outcome of a lawsuit concerning it might very well depend on the interpretation of the court. However, in this case there are a few circumstances that point to our project being fair use:</p> <ul> <li>published works and factual, non-fiction works are more likely to qualify for fair use</li> <li>the use most likely does not result in economic harm to the creator or copyright owner</li> <li>it is used to create a derivative work</li> <li>it is used for research purposes</li> <li>the downloaded dataset will not be made public</li> </ul> <p>Still, as fair use is not a well-defined category, in cases such as this, contacting the copyright/website owner might be a good idea. Also, although I am pretty confident that it was actually fair use, I am not a lawyer, so I could be completely and utterly wrong, and my reasoning may not be worth a pair of dingo‚Äôs kidneys. The point is, take the stuff I write with a grain of salt. (Taking any legal advice you get from strangers on the internet with a grain of salt might also be a good general rule.)</p> <p>Also, I am sorry about not sharing the resulting dataset. I would really like to, as it has many possibilities for research and visualization, but I don‚Äôt want to risk it. If <a href="https://digitalcommons.law.scu.edu/cgi/viewcontent.cgi?referer=https://benbernardblog.com/&amp;httpsredir=1&amp;article=2261&amp;context=historical" target="_blank" rel="noopener noreferrer">LinkedIn went after anonymous people scraping their site this seriously</a>, I would not like to be on the wrong side of <a href="https://arstechnica.com/tech-policy/2017/06/scientific-research-piracy-site-hit-with-15-million-fine/" target="_blank" rel="noopener noreferrer">academic publishers</a> and service providers.</p> <h3 id="lets-just-scrape-it">Let‚Äôs just scrape it!</h3> <p>Now that we are past the poring part, it is time to get to the actual programming. One last warning though: I am not sharing this code to use it for scraping EBSCOhost specifically - I just want to illustrate how programmatically downloading complex sites can be done.</p> <p>Scraping EBSCOhost is going to more challenging than doing the same with a simple static html resource. We will have to deal with user sessions, AJAX and such complications. In such a case, the elegant solution would be familiarizing ourselves with the site in order to replicate the necessary AJAX calls, and use a simple HTTP library such as <code class="language-plaintext highlighter-rouge">requests</code> to download make the, well, requests. In this particular case however, automating an actual web browser seemed like a much simpler (albeit probably slower, but one should not make too many requests too fast anyways) thing to do. The way EBSCOhost search works makes this strategy especially productive.</p> <p>Now would be the time to include some screenshots of the site in question, but, alas, I do not have any, and do not have access to this service anymore. However, the most important aspects about how it works (or worked in 2015) can be easily summarized. You can make a search by using a set of filters, and you will get a paginated view of the resulting articles (just like google, really). If you click on one of them, you will get its details (such as the JEL codes, yay!) - no surprises this far. The best part come now: when a result is open, there are previous and next article buttons at the bottom of the page, with which you can cycle through the <em>search results</em>! Also, the link when an article is open can be safely copied to another browser, and you get the exact same situation (same search parameters, same article).</p> <p>So in theory, the procedure should be quite simple:</p> <ol> <li>Use your favourite browser to make a search with the desired filters (say, all articles published in 2014)</li> <li>Open the first result</li> <li>Navigate to the link of the first result using an automated browser</li> <li>Keep saving the information on the current page and pressing the next button until no more articles are left</li> </ol> <p>As there are going to be plenty of exceptions (internet connection error, expired session, etc.), it might be useful to make the scraper object a <em>context manager</em>, so that all resources are closed in case of an exception without the excessive use of <code class="language-plaintext highlighter-rouge">try...catch</code> blocks. Doing so only requires us to declare an <code class="language-plaintext highlighter-rouge">__enter__</code> and an <code class="language-plaintext highlighter-rouge">__exit__</code> method. The nice thing is that the latter will be executed even if an exception occurs.</p> <p>The downloaded data is going to be stored in an SQLite database, and I am going to use <code class="language-plaintext highlighter-rouge">PhantomJS</code> automated by <code class="language-plaintext highlighter-rouge">selenium</code> as my web browser (Mr. Browser). The class starts like this:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">time</span>
<span class="kn">import</span> <span class="n">sqlite3</span>
<span class="kn">from</span> <span class="n">selenium</span> <span class="kn">import</span> <span class="n">webdriver</span>
<span class="kn">from</span> <span class="n">selenium.common.exceptions</span> <span class="kn">import</span> <span class="n">NoSuchElementException</span>
<span class="kn">from</span> <span class="n">bs4</span> <span class="kn">import</span> <span class="n">BeautifulSoup</span>
<span class="kn">import</span> <span class="n">re</span>

<span class="k">class</span> <span class="nc">EbscoScraper</span><span class="p">:</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">start_url</span><span class="p">,</span> <span class="n">database_path</span><span class="p">,</span> <span class="n">first_parsed</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>

        <span class="n">self</span><span class="p">.</span><span class="n">start_url</span> <span class="o">=</span> <span class="n">start_url</span>
        <span class="n">self</span><span class="p">.</span><span class="n">database_path</span> <span class="o">=</span> <span class="n">database_path</span>
        <span class="n">self</span><span class="p">.</span><span class="n">parsed</span> <span class="o">=</span> <span class="n">first_parsed</span>

        <span class="n">self</span><span class="p">.</span><span class="n">conn</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="n">self</span><span class="p">.</span><span class="n">c</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="n">self</span><span class="p">.</span><span class="n">mr_browser</span> <span class="o">=</span> <span class="bp">None</span>

    <span class="k">def</span> <span class="nf">__enter__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">conn</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">c</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">init_database</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">mr_browser</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">init_browser</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">parsed</span><span class="p">:</span>
            <span class="n">self</span><span class="p">.</span><span class="nf">load_next_page</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">self</span>    

    <span class="k">def</span> <span class="nf">__exit__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">exc_type</span><span class="p">,</span> <span class="n">exc_val</span><span class="p">,</span> <span class="n">exc_tb</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">conn</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span> <span class="n">self</span><span class="p">.</span><span class="n">conn</span><span class="p">.</span><span class="nf">close</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">mr_browser</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">none</span><span class="p">:</span> <span class="n">self</span><span class="p">.</span><span class="n">mr_browser</span><span class="p">.</span><span class="nf">quit</span><span class="p">()</span>
</code></pre></div></div> <p>Nothing surprising in these methods. The constructor method needs a starting URL and a file path for the database. (The optional argument is there because it often happens that the last loaded article is successfully parsed, but the next one cannot be loaded. In that case, we want to restart it from the last known url, but do not want to parse and store it again.) The <code class="language-plaintext highlighter-rouge">__enter__</code> method initializes the database connection and the (headless) browser. Finally, the <code class="language-plaintext highlighter-rouge">__exit__</code> method closes both of them.</p> <p>Next, the methods handling the database. The <code class="language-plaintext highlighter-rouge">CREATE TABLE IF NOT EXISTS</code> ensures that the same function can be used for creating a new database and reopening an existing one.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="k">def</span> <span class="nf">init_database</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">Open the connection to an sqlite database, and create the table articles
           if it does not exist.</span><span class="sh">"""</span>
        <span class="n">conn</span> <span class="o">=</span> <span class="n">sqlite3</span><span class="p">.</span><span class="nf">connect</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">database_path</span><span class="p">)</span>
        <span class="n">c</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">conn</span><span class="p">.</span><span class="nf">cursor</span><span class="p">()</span>
        <span class="n">c</span><span class="p">.</span><span class="nf">execute</span><span class="p">(</span><span class="sh">'''</span><span class="s">CREATE TABLE IF NOT EXISTS articles
                     (an INTEGER, author TEXT, title TEXT, jel TEXT, keywords TEXT)</span><span class="sh">'''</span><span class="p">)</span>
        <span class="n">conn</span><span class="p">.</span><span class="nf">commit</span><span class="p">()</span>
        <span class="nf">return </span><span class="p">(</span><span class="n">conn</span><span class="p">,</span> <span class="n">c</span><span class="p">)</span>
            
    <span class="k">def</span> <span class="nf">enter_into_db</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">dct</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">Enter data from a dictionary into the open sqlite database.</span><span class="sh">"""</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">self</span><span class="p">.</span><span class="n">c</span><span class="p">.</span><span class="nf">execute</span><span class="p">(</span><span class="sh">'''</span><span class="s">INSERT INTO articles (an, author, title, jel, keywords)
                            VALUES (?, ?, ?, ?, ?)</span><span class="sh">'''</span><span class="p">,</span> 
                            <span class="p">(</span><span class="n">dct</span><span class="p">[</span><span class="sh">'</span><span class="s">an</span><span class="sh">'</span><span class="p">],</span> <span class="n">dct</span><span class="p">[</span><span class="sh">'</span><span class="s">author</span><span class="sh">'</span><span class="p">],</span> <span class="n">dct</span><span class="p">[</span><span class="sh">'</span><span class="s">title</span><span class="sh">'</span><span class="p">],</span> <span class="n">dct</span><span class="p">[</span><span class="sh">'</span><span class="s">jel</span><span class="sh">'</span><span class="p">],</span> <span class="n">dct</span><span class="p">[</span><span class="sh">'</span><span class="s">keywords</span><span class="sh">'</span><span class="p">])</span>
                      <span class="p">)</span>
            <span class="n">self</span><span class="p">.</span><span class="n">conn</span><span class="p">.</span><span class="nf">commit</span><span class="p">()</span>
        <span class="k">except</span> <span class="n">sqlite3</span><span class="p">.</span><span class="n">OperationalError</span><span class="p">:</span>
            <span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Writing into the database has failed</span><span class="sh">'</span><span class="p">)</span>
            <span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">current url: {}</span><span class="sh">'</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">mr_browser</span><span class="p">.</span><span class="n">current_url</span><span class="p">))</span>
            <span class="k">raise</span>
</code></pre></div></div> <p>Then the methods to control the browser. Both are quite self explanatory: <code class="language-plaintext highlighter-rouge">init_browser</code> opens a browser and loads the page under <code class="language-plaintext highlighter-rouge">start_url</code>, while <code class="language-plaintext highlighter-rouge">load_next_page</code> clicks the next article button. If it cannot find the button, we are probably not on the expected page. In that case it makes a screenshot, and reraises the exception to make the scraping stop.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="k">def</span> <span class="nf">init_browser</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">Initialize the browser and open the page under start_url.</span><span class="sh">"""</span>
        <span class="n">mr_browser</span> <span class="o">=</span> <span class="n">webdriver</span><span class="p">.</span><span class="nc">PhantomJS</span><span class="p">()</span>
        <span class="n">mr_browser</span><span class="p">.</span><span class="nf">set_window_size</span><span class="p">(</span><span class="mi">1280</span><span class="p">,</span> <span class="mi">1024</span><span class="p">)</span>
        <span class="n">mr_browser</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">start_url</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">mr_browser</span>
        
    <span class="k">def</span> <span class="nf">load_next_page</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">Find and click on the next article button.</span><span class="sh">"""</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">next_btn</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">mr_browser</span><span class="p">.</span><span class="nf">find_element_by_id</span><span class="p">(</span>
                <span class="sh">'</span><span class="s">ctl00_ctl00_MainContentArea_MainContentArea_topNavControl_btnNext</span><span class="sh">'</span>
                <span class="p">)</span>
            <span class="n">next_btn</span><span class="p">.</span><span class="nf">click</span><span class="p">()</span>
        <span class="k">except</span> <span class="n">NoSuchElementException</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">mr_browser</span><span class="p">.</span><span class="n">current_url</span> <span class="o">==</span> <span class="sh">'</span><span class="s">about:blank</span><span class="sh">'</span><span class="p">:</span>
                <span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Please check your internet connection.</span><span class="sh">'</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">self</span><span class="p">.</span><span class="n">mr_browser</span><span class="p">.</span><span class="nf">save_screenshot</span><span class="p">(</span><span class="sh">'</span><span class="s">error.png</span><span class="sh">'</span><span class="p">)</span>
                <span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">The page is not as expected, screenshot saved.</span><span class="sh">'</span><span class="p">)</span>
                <span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">url: {}</span><span class="sh">'</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">mr_browser</span><span class="p">.</span><span class="n">current_url</span><span class="p">))</span>
            <span class="k">raise</span>
</code></pre></div></div> <p>We use <code class="language-plaintext highlighter-rouge">BeautifulSoup4</code> to parse the page, find the necessary elements, and extract the information. One only needs to inspect the source HTML/XML to do it. The most important fields we pull are the JEL codes (obviously), the keywords and the authors, as all of these can be used to construct various networks.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="k">def</span> <span class="nf">parse_page</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>

        <span class="n">soup</span> <span class="o">=</span> <span class="nc">BeautifulSoup</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">mr_browser</span><span class="p">.</span><span class="n">page_source</span><span class="p">)</span>

        <span class="n">dt_tags</span> <span class="o">=</span> <span class="n">soup</span><span class="p">.</span><span class="nf">find_all</span><span class="p">(</span><span class="sh">'</span><span class="s">dt</span><span class="sh">'</span><span class="p">,</span> <span class="n">attrs</span> <span class="o">=</span> <span class="p">{</span><span class="sh">'</span><span class="s">data-auto</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">citation_field_label</span><span class="sh">'</span><span class="p">})</span>
        <span class="n">field_names</span> <span class="o">=</span> <span class="p">[</span><span class="n">tag</span><span class="p">.</span><span class="n">string</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">tag</span> <span class="ow">in</span> <span class="n">dt_tags</span><span class="p">]</span>

        <span class="n">dd_tags</span> <span class="o">=</span> <span class="n">soup</span><span class="p">.</span><span class="nf">find_all</span><span class="p">(</span><span class="sh">'</span><span class="s">dd</span><span class="sh">'</span><span class="p">,</span> <span class="n">attrs</span> <span class="o">=</span> <span class="p">{</span><span class="sh">'</span><span class="s">data-auto</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">citation_field_value</span><span class="sh">'</span><span class="p">})</span>
        <span class="n">field_values</span> <span class="o">=</span> <span class="p">[</span><span class="n">tag</span><span class="p">.</span><span class="n">text</span> <span class="k">for</span> <span class="n">tag</span> <span class="ow">in</span> <span class="n">dd_tags</span><span class="p">]</span>

        <span class="n">pagedict</span> <span class="o">=</span> <span class="nf">dict</span><span class="p">(</span><span class="nf">zip</span><span class="p">(</span><span class="n">field_names</span><span class="p">,</span> <span class="n">field_values</span><span class="p">))</span>

        <span class="n">needed_fields</span> <span class="o">=</span> <span class="p">[</span><span class="sh">'</span><span class="s">Accession Number</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Author</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Title</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Descriptors</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Keywords</span><span class="sh">'</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">field</span> <span class="ow">in</span> <span class="n">needed_fields</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">field</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">pagedict</span><span class="p">.</span><span class="nf">keys</span><span class="p">():</span>
                <span class="n">pagedict</span><span class="p">[</span><span class="n">field</span><span class="p">]</span> <span class="o">=</span> <span class="sh">''</span>

        <span class="n">re_pattern</span> <span class="o">=</span> <span class="sa">r</span><span class="sh">'</span><span class="s">\([A-Z]\d\d\)</span><span class="sh">'</span>
        <span class="n">descriptors_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">item</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">re</span><span class="p">.</span><span class="nf">findall</span><span class="p">(</span><span class="n">re_pattern</span><span class="p">,</span> <span class="n">pagedict</span><span class="p">[</span><span class="sh">'</span><span class="s">Descriptors</span><span class="sh">'</span><span class="p">])]</span>
        <span class="n">descriptors</span> <span class="o">=</span> <span class="sh">'</span><span class="s">; </span><span class="sh">'</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">descriptors_list</span><span class="p">)</span>
        <span class="n">keywords</span> <span class="o">=</span> <span class="sh">'</span><span class="s"> </span><span class="sh">'</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">pagedict</span><span class="p">[</span><span class="sh">'</span><span class="s">Keywords</span><span class="sh">'</span><span class="p">].</span><span class="nf">split</span><span class="p">())</span>
        
        <span class="n">finaldict</span> <span class="o">=</span> <span class="p">{</span><span class="sh">'</span><span class="s">an</span><span class="sh">'</span><span class="p">:</span> <span class="n">pagedict</span><span class="p">[</span><span class="sh">'</span><span class="s">Accession Number</span><span class="sh">'</span><span class="p">],</span>
                     <span class="sh">'</span><span class="s">author</span><span class="sh">'</span><span class="p">:</span> <span class="n">pagedict</span><span class="p">[</span><span class="sh">'</span><span class="s">Author</span><span class="sh">'</span><span class="p">],</span>
                     <span class="sh">'</span><span class="s">title</span><span class="sh">'</span><span class="p">:</span> <span class="n">pagedict</span><span class="p">[</span><span class="sh">'</span><span class="s">Title</span><span class="sh">'</span><span class="p">],</span>
                     <span class="sh">'</span><span class="s">jel</span><span class="sh">'</span><span class="p">:</span> <span class="n">descriptors</span><span class="p">,</span>
                     <span class="sh">'</span><span class="s">keywords</span><span class="sh">'</span><span class="p">:</span> <span class="n">keywords</span>
                     <span class="p">}</span>
                     
        <span class="k">return</span> <span class="n">finaldict</span>
</code></pre></div></div> <p>Finally, we go through all of the pages and save the relevant information one by one. We do this in a loop.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="k">def</span> <span class="nf">parse_n_pages</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">num_of_pages</span><span class="p">,</span> <span class="n">interval</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">Load and parse num_of_pages search results, including the current page.</span><span class="sh">"""</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_of_pages</span><span class="p">):</span>

            <span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>

            <span class="n">dct</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">parse_page</span><span class="p">()</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nf">all</span><span class="p">([</span><span class="n">key</span> <span class="o">==</span> <span class="sh">''</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">dct</span><span class="p">.</span><span class="nf">keys</span><span class="p">()]):</span>
                <span class="n">self</span><span class="p">.</span><span class="nf">enter_into_db</span><span class="p">(</span><span class="n">dct</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">No useful information on {}.</span><span class="sh">'</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">mr_browser</span><span class="p">.</span><span class="n">current_url</span><span class="p">))</span>

            <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">num_of_pages</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">self</span><span class="p">.</span><span class="nf">load_next_page</span><span class="p">()</span>

            <span class="nf">if </span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="mi">50</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">{} of {} articles parsed and saved.</span><span class="sh">'</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">num_of_pages</span><span class="p">))</span>

            <span class="n">time</span><span class="p">.</span><span class="nf">sleep</span><span class="p">(</span><span class="nf">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">t0</span> <span class="o">+</span> <span class="n">interval</span> <span class="o">-</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()))</span>
        
        <span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Finished saving and parsing all {} articles.</span><span class="sh">'</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">num_of_pages</span><span class="p">))</span>
</code></pre></div></div> <p>The scraper is ready for action now. It can be used like this:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="nc">EbscoScraper</span><span class="p">(</span><span class="n">start_url</span><span class="p">,</span> <span class="n">database_path</span><span class="p">)</span> <span class="k">as</span> <span class="n">mr_scraper</span><span class="p">:</span>
    <span class="n">mr_scraper</span><span class="p">.</span><span class="nf">parse_n_pages</span><span class="p">(</span><span class="n">num_of_pages</span><span class="p">)</span>
</code></pre></div></div> <p>Let it run for some hours, restart it a few times if it stops, and you will have the data in a convinient SQLite format. In our case, data on 37320 articles (all of the EBSCO search results for articles published in 2014).</p> <p>In the next part of the series, I will continue with making a network out of this data, and visualizing that network. Stay tuned!</p> </article> </div> </div> <footer class="sticky-bottom mt-5"> <div class="container"> ¬© Copyright 2025 Martin Stancsics. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> </body> </html>